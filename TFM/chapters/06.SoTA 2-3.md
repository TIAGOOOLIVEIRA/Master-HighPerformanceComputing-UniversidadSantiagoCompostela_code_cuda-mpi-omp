# Architecture Design Decisions for Ray Data Preparation and HPC on Cloud
This work aims to evaluate a two phased architecture approach as an framework to address the current needs for highly scalable pre-processing, storage and specific numeric simulation purposes.



<img src="../images/AWS_VectorMachine_Ray-HPC-architecture.drawio.svg" alt="AWS Architecture" width="500">

**Figure 4:** Here the end‐to‐end pipeline architecture into a clean two‐phase pattern (Ray to HPC)


Modular components is a key best practice for High-Performance Computing (HPC) workloads on Amazon Web Services (AWS), contributing to improved operational efficiency and reduced manual intervention. This approach is crucial for managing complex applications and facilitating continuous development practices in the cloud environment.

The Figure 4 shows the end‐to‐end pipeline, whereas following further view into the two‐phase pattern:

* **Phase 1** is all about data prep with Ray. Raw FASTQ or PDB files land in an S3 “data lake,” cataloged by Lake Formation, then ingested and sharded by Ray Data or AWS Glue jobs running on GPU‐enabled EC2/EKS clusters. Ray’s tasks and actors spin up ProtBERT (or other UDFs) to tokenize, vectorize, and write embeddings back to S3 or an FSx for Lustre filesystem.

* **Phase 2** picks up those embeddings for heavy numerical work: a Slurm/MPI‐based ParallelCluster runs on C5n/G5/P4d instances with EFA networking and FSx for Lustre I/O. Your `mpi_dna_seq` or GROMACS jobs consume the compact `(N×D)` arrays and produce simulation outputs.

On the IaC side, both stacks — Ray autoscaler clusters and ParallelCluster templates — are codified in CloudFormation (or CDK/Terraform) and deployed via CodePipeline → CodeBuild. Auto-scaling policies, CloudWatch dashboards, and Lambda-backed alarms complete the loop, giving you repeatable, monitored, hybrid Ray + HPC workflows entirely managed as code.


## Data centric - Data backbone

<img src="../images/DataCentric-DataBackbone-Strategy.png" alt="Data backbone" width="500">

**Figure 5:** Trend in the cloud industry to ties easily integration between different processing engines and capabilities to simplify specially data exchange for one to feed another: HPC <-> AI


### Storage - Data Structure and Format

The Figure 5 embodies a classic **Data-Centric Pipeline** design pattern, where a central “Data Backbone” feeds and is fed by two complementary compute domains:

1. **Phase 1: Ray-Powered Preprocessing**

   * **Pattern:** “Last-Mile ETL,” where raw, large-scale source streams (Research, IoT, ERP/CRM, Edge) are ingested, cleansed, and vectorized as embeddings in a distributed, autoscaling Ray cluster.
   * **Enablers:**

     * **S3 Data Lake + Lake Formation** for governed, centralized storage and metadata.
     * **Ray Data & Ray AIR** on GPU-backed EC2/EKS for tokenization, feature extraction, and AI-driven embedding pipelines.
     * **AWS Glue** or **DataSync** for hybrid on-prem to cloud data flows.
     * **CloudFormation/CDK** for spinning up the Ray autoscaler as IaC, tied into CodePipeline for CI-CD.

2. **Phase 2: HPC-Driven Simulation**

   * **Pattern:** “Distributed Batch Compute,” where the compact embeddings drive high-fidelity simulations or MPI jobs on an EFA and FSx - accelerated ParallelCluster.
   * **Enablers:**

     * **AWS ParallelCluster** (IaC via CloudFormation) to provision Slurm head nodes, GPU/CPU fleets, EFA networking, and FSx Lustre.
     * **EFA + 100 Gbps C5n/G5/P4d instances** for low-latency MPI communication.
     * **Spot Fleet & Auto-Scaling** policies tied to Slurm queue metrics via CloudWatch + Lambda.

3. **The Hybrid Feedback Loop**

   * **Pattern:** “Model Improvement Loop,” where simulation outputs (e.g., novel molecular conformers or refined genomic predictions) are fed back through the Ray pipeline for retraining or fine-tuning AI models—closing the loop between data prep and compute.
   * **Enablers:**

     * **Event-Driven Orchestration** (EventBridge → Lambda → Ray job submit) to trigger retraining when new simulation results arrive.
     * **SageMaker Pipelines** (or Ray Tune) to perform hyperparameter optimization on updated embeddings.

---

### State-of-the-Art Trends & Cloud Industry Direction

* **Data Mesh & Lakehouse Convergence:** Rather than monolithic data lakes, modern architectures favor **domain-oriented data products** that register in a central catalog but remain owned and produced by each team. AWS Lake Formation, Databricks Unity Catalog, and MS Fabric’s OneLake all push this model forward.

* **Function-to-Data Over Data-to-Function:** With **Lambda for HPC** (e.g., HPC-optimized containers in Lambda), serverless bursts can handle stateless “last-mile” tasks, seamlessly integrating with Stateful Slurm jobs—precisely the **Mashup** pattern.

* **Cloud-Native HPC Fabrics:** AWS’s Elastic Fabric Adapter and FSx for Lustre are being joined by **Azure’s HPC Cache** and **Google’s Filestore Extreme** to deliver on-demand, low-latency fabrics. Microsoft’s new **Discovery Platform** similarly unifies large-scale data exploration with integrated compute kernels, echoing your Data Backbone vision.

* **Unified AI + HPC Frameworks:** Ray’s rise—now baked into AWS SageMaker HyperPod, Databricks Runtime, and MS Azure ML—reflects the industry trend toward **single‐pane orchestration** for AI & HPC, reducing the impedance mismatch between ML pipelines and batch simulations.


In sumary, the two-phase Ray → HPC loop sits squarely at the forefront of cloud architecture: it leverages managed, scalable storage; unifies heterogeneous compute; wires up serverless orchestration; and embodies the feedback patterns that next-generation AI/HPC workloads demand.
