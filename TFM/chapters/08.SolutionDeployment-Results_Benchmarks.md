# Ray load DNA sequencing file to vectorize-generate embeddings to downstream the output in the HPC cluster


## Recipe: Two-Phase Pipeline on AWS

**Phase 1 – Ray-based Embedding Generation**
You’ll first package `dna_embeddings.py` into a Docker image with GPU support (e.g. `rayproject/ray-ml:latest-gpu`). Then, check that image and your script into GitHub under `/src`. In your IaC repo you include the **Ray autoscaler** config (`ray-cluster-example-full.yaml`) as part of a CloudFormation stack—using a custom resource or a simple CodeBuild project that runs `ray up` automatically whenever you push to `main`. With that in place, your CI/CD pipeline (CodePipeline → CodeBuild) will:

1. Build & push the GPU‐enabled Docker image to ECR.
2. Deploy or update the Ray cluster via the autoscaler YAML (executed from CodeBuild).
3. SSH into the head node to kick off your embedding job:

   ```bash
   ray submit ray-cluster-example-full.yaml dna_embeddings.py \
     -- args --input s3://my-bucket-xxx/raw/genomes/sample1.fq \
            --output s3://my-bucket-xxx/embeddings/sample1.parquet
   ```

By embedding S3 paths, Ray tasks will spin up GPU actors on the worker nodes, load your FASTQ, apply ProtBERT or other encoders, and write out Parquet shards of `(N × D)` floats.

**Phase 2 – MPI-Driven Numeric Simulation**
While Phase 1 runs, in parallel you maintain a second CloudFormation template for **AWS ParallelCluster** (or use the built-in `pcluster` CLI). That template (e.g. `parallelcluster-config.yaml`) declares your Slurm head node, GPU/CPU compute fleet, FSx for Lustre, EFA networking, and a bootstrap action to `git clone` your repo and compile `mpi_dna_seq.cpp`. Once embeddings land in S3/FSx, Slurm users can submit:

```bash
sbatch run_sim.sh sample1.parquet
```

where `run_sim.sh` looks like:

```bash
#!/bin/bash
#SBATCH --job-name=dna-sim
#SBATCH --ntasks=16
#SBATCH --gres=gpu:1
srun ./mpi_dna_seq mpi_input --embeddings s3://my-bucket/embeddings/sample1.parquet
```

Under the hood, the MPI code reads the Parquet vectors (via Arrow or custom reader), performs the simulation kernel-solver, and writes out results to Lustre.

**Automation & Monitoring**
Tie both stacks into a single **CodePipeline**:

* **Source**: GitHub
* **Build**: CodeBuild for Docker + CloudFormation deploy
* **Deploy**: CloudFormation update stacks for Ray and ParallelCluster

Use CloudWatch alarms on Ray’s autoscaler logs (Dashboard port 8265) and Slurm job throughput. Metrics like pending tasks or job queue length can trigger Lambda to scale your HPC fleet up/down.

---

### The execution of the applications Ray -> HPC-MPI

```bash
# Phase 1: Submit embedding job via Ray CLI
ray submit ray-cluster-example-full.yaml dna_embeddings.py \
  --args \
    --input s3://genomics-data/raw/sample1.fq \
    --output s3://genomics-data/embeddings/sample1.parquet \
    --batch-size 1024 --num-gpus 1

# Phase 2: Submit MPI simulation job on ParallelCluster
sbatch << 'EOF'
#!/bin/bash
#SBATCH --job-name=dna-sim
#SBATCH --ntasks=32
#SBATCH --gres=gpu:1
#SBATCH --time=02:00:00
module load mpi
srun ./mpi_dna_seq \
  --embeddings /fsx/embeddings/sample1.parquet \
  --output /fsx/results/sample1_sim.nc
EOF
```

This snippet demonstrates how you invoke each phase from your head nodes—embedding generation via Ray on GPU nodes, followed by an HPC-scale MPI job consuming those embeddings. Continuous deployment and CloudWatch monitoring then ensure the two-phase pipeline stays healthy and cost-efficient.

### Ray application to ensure GPU cards will be used for acceleration in the embeddings generation

```python
# code/ray_app/dna_embeddings.py
    ...
    embedded = ds.map_batches(
        ComputeDNAEmbeddings,
        batch_size=32,               # tune for memory
        fn_constructor_kwargs={
            "seq_col": "sequence",
            "model_name": HF_MODEL,
            "device": "cpu",         # or "cuda"
        },
        concurrency=4,               # number of actors
        num_gpus=2,                  # set >0 if using GPUs
    )
    embedded.write_parquet(OUTPUT_PATH, try_create_dir=True)
    print("Embeddings written to", OUTPUT_PATH)
```