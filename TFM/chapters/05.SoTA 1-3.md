The two-phase pipeline - as the Figure 3 shows - moving from data preprocessing to feeding a numerical simulation, is a common and critical workflow in HPC and Machine Learning, especially when dealing with large, complex datasets like genomic sequences or simulation outputs involves overcoming challenges related to data volume, variety, and velocity.

The first phase focuses on preparing vast, often raw and unstructured, datasets into a clean, transformed, and properly formatted state suitable for numerical simulation or complex model training.

The second phase involves executing the core computational models or algorithms using the preprocessed data. These applications are often "compute-intensive" and require significant parallel processing capabilities.

Here is an elaboration of the main steps and tasks for each phase, highlighting where Ray and AWS cloud-based HPC services fit in.

<img src="../images/Generalized_data-pipeline.drawio.png" alt="Common ETL" width="500">

**Figure 3:** Processes and Storage schema for the most common steps in a two-phased data pipeline pattern



**Most common scientific data pre-processing steps needs before numerical simulation needs:**


 | Step                                    | MD (Molecular Dynamics)                                                                                    | CFD (Computational Fluid Dynamics)                                                                                            | FFT (Spectral Analysis)                                        | Genomics / Protein Sequencing                                                                     |
| --------------------------------------- | ---------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| **Validation & Cleaning**               | • Fix missing atoms/residues<br>• I/O: small text files, many parses<br>• CPU‐bound serial cleanup         | • Geometry repair, ensure watertight meshes<br>• I/O: CAD formats heavy parsing<br>• Memory: storing geometry                 | • Window selection, denoising<br>• CPU‐light, GPU‐trivial      | • Sequence QC, adapter trimming<br>• I/O: FASTQ large files<br>• Irregular string parses          |
| **Setup & Discretization**              | • Box definitions, coordinate conversion<br>• Memory: atom arrays<br>• GPU: cell lists for neighbor search | • Mesh generation (unstructured/tet/hexa)<br>• Compute: meshing algorithms (serial/p threaded)<br>• Memory: large mesh in RAM | • Signal framing, padding<br>• Low compute; trivial memory     | • k-mer indexing, suffix arrays<br>• Memory: huge hash tables<br>• Parallel: distributed indexing |
| **Partitioning & Chunking**             | • Split trajectory into frame blocks<br>• I/O throughput: binary streams<br>• Memory: chunk fits node      | • Domain decomposition subdomains<br>• I/O: writing mesh partitions<br>• Compute: balance & load distribution                 | • Segmenting time series<br>• Minimal I/O; in‐memory slicing   | • Batch reads for alignment<br>• I/O: S3/FSx streaming needed                                     |
| **Computation**                         | • Integration kernels (Verlet)<br>• GPU: OpenMM, CUDA acceleration<br>• Compute‐bound with data locality   | • Solver loops (Navier–Stokes)<br>• Memory‐bandwidth bound<br>• GPU: CuPHY or OpenCL solvers                                  | • FFT kernels (cuFFT / FFTW)<br>• Highly optimized GPU libs    | • Alignment (BWA, Bowtie) or assembly<br>• Irregular memory access; vectorization possible        |
| **Feature Extraction & Transformation** | • Compute distances, RDFs, contact maps<br>• CPU/GPU transforms<br>• Memory: distance matrices             | • Extract forces, pressure coefficients<br>• CPU heavy reductions<br>• Memory: scalar fields                                  | • Spectral feature extraction<br>• GPU trivial with transforms | • Variant calling, motif detection<br>• Compute: HMMs; I/O: reference lookups                     |
| **Serialization & Storage**             | • HDF5 or NetCDF writes<br>• I/O bound, parallel HDF5 recommended                                          | • VTK, CGNS, NetCDF outputs<br>• Storage: large binary dumps                                                                  | • Binary arrays, Parquet<br>• Minimal size                     | • Parquet/CSV for variants<br>• Metadata catalogs, many small files                               |

**Table 1:** Phase 1 regular steps for some scientific modeling & simulation context domain


### Objectives

Building on the common Phase 1 workflows summarized in Table 1 — where every discipline from Molecular Dynamics to FFT shares validation, discretization, partitioning, computation, feature extraction, and serialization steps — this study will focus on a simple Genomics/Protein Sequencing task pipeline. DNA sequencing uniquely combines ultra‐large FASTQ inputs, irregular string parsing for quality filtering, massive in-memory hash tables for k-mer indexing, distributed I/O demands (S3/FSx streaming), and vectorizable yet irregular - access alignment kernels. By choosing this domain, both validation and stress-test for the two‐phase architecture: Ray-powered, GPU-accelerated preprocessing in Phase 1, followed by MPI/Slurm‐driven numerical simulations in Phase 2. This focus ensures that this solution addresses one of the most data-intensive and computationally diverse scientific pipelines in practice.

Therefore with an on-purpose emphasis on the Phase 1, where Ray Tech-stack takes responsibility on that part where for a regular HPC application is not the best fit, given the new State-of-the-art for data preparation framework brings the value.


## Phase 1: Data Preparation for Genomic and Proteomic Workflows

Before feeding data into high-performance simulators (Phase 2), we must transform raw sequencing outputs into clean, structured, high-dimensional vectors. Phase 1 uses distributed frameworks like Ray to orchestrate these steps at scale.


## 1. Ingestion and Quality Control

First, terabytes of raw FASTQ reads — complete with base calls and quality scores — are ingested, often from on-premises archives into Amazon S3 via AWS DataSync [15],[7]. Immediately upon arrival, a quality-control pass removes low-quality bases (for example, trimming any read positions with median quality below 20) and clips adapter sequences. Tools like FastQC flag problematic regions so that downstream analyses start from the cleanest possible dataset [10].

---

## 2. Cleaning and Error Correction

Next, the pipeline “wrangles” the data correcting sequencing errors that can range from 1–2 percent on Illumina platforms [5] up to 50 percent in homopolymer stretches. Spectrum-based error-correction algorithms smooth out miscalled bases, and duplicate reads (often PCR artifacts) are collapsed so that each unique genomic coordinate contributes only once [15]. The result is a de-noised, high-fidelity read set that greatly improves assembly and simulation accuracy.

---

## 3. Format Conversion and Alignment

With clean reads in hand, we convert between formats — FASTA for raw sequences, SAM/BAM for alignments — leveraging the compressed, indexed structure of BAM to handle massive alignment outputs efficiently [11]. Reads are then mapped to a reference genome (or assembled de novo when no reference is available) using tools like BWA or Bowtie2 for DNA and specialized mappers for RNA-Seq. 

---

## 4. Feature Extraction and Embedding Generation

Once aligned or assembled, biological features can be extracted: ORFs, motifs, restriction sites, and variants (SNPs/indels) via pipelines such as MeDUSA [10]. In parallel, a high-level features decomposing task reads into k-mers or reduced alphabets for proteins [16] — and then vectorize them. For example, k-mers become high-dimensional embeddings, and DNABERT applies Transformer based pretraining to learn “DNA language” and produce context rich sequence embeddings [6]. Additional augmentation can draw in public databases (GenBank, Ensembl) or pathway resources (KEGG, MetaCyc) to enrich each record [9], [20], [4].

---

## 5. Sharding and Simulation-Ready Output

To feed HPC solvers efficiently, the embedding arrays are partitioned into shards that respect the data-access patterns of MPI/Slurm workflows [15]. This careful partitioning ensures balanced workloads across nodes and minimizes cross-node I/O. The final output—whether NetCDF for MD or matrix inputs for spectral solvers—is optimized for the high-throughput file systems (FSx for Lustre) and network fabrics (EFA) used in Phase 2.

---

## Ray’s Role in Phase 1

Throughout these stages, **Ray** acts as the unifying, scalable compute fabric. Its **tasks** and **actors** parallelize ingestion, cleaning, and featurization without changing a line of business code. **Ray Data** offers in-memory pipelines that stream records across CPUs and GPUs, overlapping I/O and compute to minimize idle time. With **Ray AIR** preprocessors, feature scalers, categorical encoders, and token-to-embedding transforms can be implemented in a single distributed system—eliminating framework sprawl. By the end of Phase 1, Ray has converted raw, heterogeneous sequencing files into the compact, shardable numeric arrays that Phase 2’s HPC simulations crave.
