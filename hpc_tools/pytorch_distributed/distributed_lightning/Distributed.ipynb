{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce86f47-985f-4c09-8f06-6a5e17742740",
   "metadata": {},
   "source": [
    "## Deliverable 2: DISTRIBUTED - Distributed implementation (Due 14th october 2024)\n",
    "\n",
    "After that, you need to parallelize that code using the native support for Distributed Training in Pytorch or Lightning. The distributed training tool (native or Lightning) and the strategy (DP, DDP, Zero, FSDP) for work distribution depend on you. You can actually experience several if you feel like it.\n",
    "\n",
    "Once you have the implementation, you have to test it and measure execution times as you did with the BASELINE. Report just times or the output of a profiling time as you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19324c98-8f54-489b-b8b7-3cb2efa48f49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 14 23:17:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              34W / 250W |      4MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCIE-40GB          On  | 00000000:98:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              36W / 250W |      4MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3942b55-2df5-4a03-a981-3010e06ad0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (4.30.2)\n",
      "Requirement already satisfied: datasets in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (2.13.2)\n",
      "Requirement already satisfied: torch in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: filelock in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (6.7.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: requests in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: pandas in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: xxhash in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: zipp>=0.5 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0; python_version < \"3.8\" in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: wheel in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch) (0.42.0)\n",
      "Requirement already satisfied: setuptools in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\"->torch) (47.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d2c8dd-ebb9-4bf1-9358-57608f792aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 23:17:58.461034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-14 23:17:58.600304: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-14 23:17:59.583721: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/gr/lib64\n",
      "2024-10-14 23:17:59.583928: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/gr/lib64\n",
      "2024-10-14 23:17:59.583936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a512b58c-28ba-4a52-867c-effeaab85733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ulc/cursos/curso355/.cache/huggingface/datasets/parquet/plain_text-7d14fdbf55ecc00c/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0499d168459143d3bb0390ad4c604e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load SQuAD dataset\n",
    "squad_dataset = load_dataset('squad')\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92597fc6-8edc-413d-aad4-e7b676a125f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride.\n",
    "    # This results in one example possible giving several features when a context is long,\n",
    "    # each of those features having a context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",  # truncate context, not the question\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context.\n",
    "    # This will help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "058702ed-bc38-469d-9b88-dcc0685badfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ulc/cursos/curso355/.cache/huggingface/datasets/parquet/plain_text-7d14fdbf55ecc00c/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-aff7705da5561f56.arrow\n",
      "Loading cached processed dataset at /home/ulc/cursos/curso355/.cache/huggingface/datasets/parquet/plain_text-7d14fdbf55ecc00c/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-1296e189dc3ae491.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply the function to our data\n",
    "tokenized_datasets = squad_dataset.map(prepare_train_features, batched=True, remove_columns=squad_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb8e7a8-8f96-45c6-80a3-1aa53dfd1938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">25</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>args=training_args,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>train_dataset=tokenized_datasets[<span style=\"color: #808000; text-decoration-color: #808000\">\"train\"</span>],                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>eval_dataset=tokenized_datasets[<span style=\"color: #808000; text-decoration-color: #808000\">\"validation\"</span>],                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>25 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>tokenizer=tokenizer,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span>)                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages/transformer</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">s/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">431</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 428 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Using --sharded_ddp xxx together with --fsdp is not possible, deact</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 429 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 430 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> args.parallel_mode != ParallelMode.DISTRIBUTED:                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 431 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Using sharded DDP only works in distributed training.\"</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 432 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> is_fairscale_available():                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 433 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ImportError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Sharded DDP training requires fairscale: `pip install</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 434 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> ShardedDDPOption.SIMPLE <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> args.sharded_ddp <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> FullyShardedDDP <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Using sharded DDP only works in distributed training.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m25\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   \u001b[0margs=training_args,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0mtrain_dataset=tokenized_datasets[\u001b[33m\"\u001b[0m\u001b[33mtrain\u001b[0m\u001b[33m\"\u001b[0m],                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   \u001b[0meval_dataset=tokenized_datasets[\u001b[33m\"\u001b[0m\u001b[33mvalidation\u001b[0m\u001b[33m\"\u001b[0m],                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m25 \u001b[2m│   \u001b[0mtokenizer=tokenizer,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m)                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/mnt/netapp2/Store_uni/home/ulc/cursos/curso355/mypython/lib/python3.7/site-packages/transformer\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33ms/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m431\u001b[0m in \u001b[92m__init__\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 428 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUsing --sharded_ddp xxx together with --fsdp is not possible, deact\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 429 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 430 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m args.parallel_mode != ParallelMode.DISTRIBUTED:                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 431 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mUsing sharded DDP only works in distributed training.\u001b[0m\u001b[33m\"\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 432 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[95mnot\u001b[0m is_fairscale_available():                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 433 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mImportError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mSharded DDP training requires fairscale: `pip install\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 434 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melif\u001b[0m ShardedDDPOption.SIMPLE \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m args.sharded_ddp \u001b[95mand\u001b[0m FullyShardedDDP \u001b[95mis\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mUsing sharded DDP only works in distributed training.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define training arguments for distributed training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",               # Output directory for model checkpoints\n",
    "    evaluation_strategy=\"epoch\",          # Evaluate after each epoch\n",
    "    learning_rate=3e-5,                   # Learning rate\n",
    "    per_device_train_batch_size=16,       # Batch size per device (per GPU)\n",
    "    per_device_eval_batch_size=16,        # Batch size for evaluation per device\n",
    "    num_train_epochs=3,                   # Number of training epochs\n",
    "    weight_decay=0.01,                    # Weight decay to prevent overfitting\n",
    "    logging_dir=\"./logs\",                 # Directory for storing logs\n",
    "    logging_steps=10,                     # Log every 10 steps\n",
    "    fp16=True,                            # Enable mixed precision training\n",
    "    report_to=\"none\",                     # Disable reporting to any external platforms (like WandB)\n",
    "    dataloader_num_workers=4,             # Number of workers for data loading\n",
    "    gradient_accumulation_steps=1,        # Accumulate gradients before optimizing\n",
    "    sharded_ddp=\"simple\",                 # Enable simple distributed data parallelism (to shard the optimizer states)\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa45897-2b36-4023-a210-3fcb5acf06f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba796799-0698-4812-9288-ee419df81577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
