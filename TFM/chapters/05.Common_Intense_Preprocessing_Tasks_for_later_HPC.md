In a general purpose data pipeline, where the first phase mainly data parsing, transformation and enhancement are the tasks leveraged [WiP]

<img src="../images/Generalized_data-pipeline.drawio.svg" alt="Common ETL" width="500">


**Most common scientific data pre-processing steps needs before numerical simulation needs:**


 | Step                                    | MD (Molecular Dynamics)                                                                                    | CFD (Computational Fluid Dynamics)                                                                                            | FFT (Spectral Analysis)                                        | Genomics / Protein Sequencing                                                                     |
| --------------------------------------- | ---------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| **Validation & Cleaning**               | • Fix missing atoms/residues<br>• I/O: small text files, many parses<br>• CPU‐bound serial cleanup         | • Geometry repair, ensure watertight meshes<br>• I/O: CAD formats heavy parsing<br>• Memory: storing geometry                 | • Window selection, denoising<br>• CPU‐light, GPU‐trivial      | • Sequence QC, adapter trimming<br>• I/O: FASTQ large files<br>• Irregular string parses          |
| **Setup & Discretization**              | • Box definitions, coordinate conversion<br>• Memory: atom arrays<br>• GPU: cell lists for neighbor search | • Mesh generation (unstructured/tet/hexa)<br>• Compute: meshing algorithms (serial/p threaded)<br>• Memory: large mesh in RAM | • Signal framing, padding<br>• Low compute; trivial memory     | • k-mer indexing, suffix arrays<br>• Memory: huge hash tables<br>• Parallel: distributed indexing |
| **Partitioning & Chunking**             | • Split trajectory into frame blocks<br>• I/O throughput: binary streams<br>• Memory: chunk fits node      | • Domain decomposition subdomains<br>• I/O: writing mesh partitions<br>• Compute: balance & load distribution                 | • Segmenting time series<br>• Minimal I/O; in‐memory slicing   | • Batch reads for alignment<br>• I/O: S3/FSx streaming needed                                     |
| **Computation**                         | • Integration kernels (Verlet)<br>• GPU: OpenMM, CUDA acceleration<br>• Compute‐bound with data locality   | • Solver loops (Navier–Stokes)<br>• Memory‐bandwidth bound<br>• GPU: CuPHY or OpenCL solvers                                  | • FFT kernels (cuFFT / FFTW)<br>• Highly optimized GPU libs    | • Alignment (BWA, Bowtie) or assembly<br>• Irregular memory access; vectorization possible        |
| **Feature Extraction & Transformation** | • Compute distances, RDFs, contact maps<br>• CPU/GPU transforms<br>• Memory: distance matrices             | • Extract forces, pressure coefficients<br>• CPU heavy reductions<br>• Memory: scalar fields                                  | • Spectral feature extraction<br>• GPU trivial with transforms | • Variant calling, motif detection<br>• Compute: HMMs; I/O: reference lookups                     |
| **Serialization & Storage**             | • HDF5 or NetCDF writes<br>• I/O bound, parallel HDF5 recommended                                          | • VTK, CGNS, NetCDF outputs<br>• Storage: large binary dumps                                                                  | • Binary arrays, Parquet<br>• Minimal size                     | • Parquet/CSV for variants<br>• Metadata catalogs, many small files                               |




### Objectives
This work aims to validate the two phased approach on a specific step pre-processing for DNA-Sequencing needs. Where data ingestion, parsing, transformation, encoding are performed in a Ray distributed cluser, and simulation are done in a HPC cluster



For DNA Genomics Protein Sequencing data pipelines, the data preparation phase (Phase 1) is crucial for transforming raw, complex, and voluminous data into a clean, structured, and vectorized format suitable for numerical simulations. This phase involves several common steps, often leveraging distributed computing frameworks like Ray.
Here are the main steps and tasks for data preparation:

1. Data Ingestion and Initial Quality Control:
Data Gathering and Storage: Raw molecular data, such as DNA, RNA, or protein sequences, are initially collected [@park2024survey] [@sun2021improve]
- Next-generation sequencing (NGS) technologies generate vast amounts of data, ranging from gigabases to terabases of sequence data per investigator, typically in FASTQ format, which includes both base calls and associated quality scores [@mandoiu2016computational]. For large genomic datasets, which can be 0.5-1TB per sample, this data is often transferred from on-premises storage to cloud services like Amazon S3 using tools such as AWS DataSync [@khanuja2022applied] [@roy2017massively]
- Initial Quality Assessment (QC): Before any analysis, raw reads undergo quality control to remove low-quality sequences and potential artifacts. This includes trimming base calls below a predefined quality threshold (e.g., median quality below 20) and removing platform-specific adapter sequences. Tools like FastQC can be used for sequencing quality assessment  [@mandoiu2016computational]

2. Data Cleaning and Error Correction:
- Data Wrangling: This involves exploring, understanding, and modifying raw data to prepare it for subsequent model training or building
- Sequencing Error Correction: NGS data is prone to errors (e.g., 1-2% for Illumina sequencers [@jammula2015parallel], or up to 50% in homopolymeric regions for 454/Roche GS FLX). These errors can significantly impact the quality of assembled contigs and lead to inaccurate downstream analyses. Error correction methods, such as Spectrum-based Error Correction (SbEC), are applied to mitigate these issues. The goal is to produce a corrected data set that is less noisy and more realistic, thereby improving the performance of subsequent simulations
- Duplicate Removal and Filtering: Processes are implemented to eliminate PCR and sequencing duplicates, often by retaining only one mapped read per unique genomic location [@roy2017massively]

3. Data Transformation and Pre-processing:
- Format Conversion: Biological sequences often need to be converted between various formats like FASTA, SAM (Sequence Alignment/Map), and BAM (Binary Alignment/Map) [@mukhopadhyay2018basic]
- BAM files are typically used for their compressed, binary, and indexed nature, making them more manageable for large alignment results
- Sequence Mapping/Alignment: A computationally intensive task where short sequencing reads are aligned to a reference genome [@mukhopadhyay2018basic]
- Common tools include BWA, Bowtie/Bowtie2, TopHat/TopHat2, GMAP/GSNAP, and RMAP. For RNA-Seq, this can involve mapping to a reference genome or transcriptome
- Assembly (De Novo): If a reference genome is unavailable, reads are "pieced together" to form gene and transcript models
- Overlaps between reads provide critical clues for local assembly. Examples of assemblers include SOAPdenovo-Trans and Trinity
- Genomic Feature Extraction: This step involves identifying specific biological features. For instance, processes can include predicting open reading frames (ORFs) , extracting motifs, detecting restriction enzyme sites, performing gene prediction, or identifying genomic variants like Single Nucleotide Polymorphisms (SNPs) and insertions/deletions (indels). Pipelines like MeDUSA and MeQA are designed for comprehensive analysis of DNA methylation sequencing (MeDIP-seq) data, from mapping to differential methylation identification [@mandoiu2016computational]

4. Feature Engineering, Vectorization, and Embedding Extraction:
- Feature Engineering: This is a key step where raw data is transformed into "high-level features" that can be interpreted and used by machine learning models or numerical simulations [@salomon2018neural] [@pumperla2023learning]
- For DNA sequences, this commonly involves decomposing reads into k-mers (substrings of length k) [@jammula2015parallel]. For protein sequences, methods can include plugging in a reduced alphabet to enhance sensitivity in similarity searches [@selvitopi2022extreme]
- Vectorization and Embedding Extraction: The transformed features are then converted into a numerical representation, such as vectors or embeddings. For example, k-mers can be converted into high-dimensional vectors (embeddings) [LSHvec, previous turn]. DNABERT leverages Transformer models to develop a general understanding of "DNA language" through "self-supervised pre-training" and then applies this to specific tasks by generating "global contextual embedding" of input sequences [@ji2021dnabert]
- This numerical format is essential for feeding data into machine learning models [@pumperla2023learning]
- Dataset Enhancement/Augmentation: The primary dataset can be enriched by integrating information from additional sources, such as public databases like GenBank [@liu2023codonbert] or Ensembl [@zhou2013motex] [@gresova2023genomic], or specialized protein/metabolic databases like KEGG and MetaCyc [@mukhopadhyay2018basic]


5. Data Partitioning and Preparation for Simulation:
- Distributed Data Preparation: For extremely large datasets, data is often partitioned or sharded [@roy2017massively]
- This is vital for distributing the computational workload across multiple processing units, which is a common practice in HPC environments. Careful partitioning is necessary to maintain accuracy in results, especially given the varying data access behaviors and dependencies of different genomic analysis programs
- Final Output Format: The culmination of Phase 1 is data in a format ready to be consumed by the numerical simulation application. For instance, Molecular Dynamics simulation data might be pre-processed into NetCDF files, optimized for efficient data I/O by HPC libraries [@wilson2016experiences]

Ray's Role in Phase 1:
Ray is well-suited for Phase 1 activities, particularly due to its capabilities for distributed data processing [572, previous turn].
- Unified and Scalable Compute: Ray acts as a unified compute framework that allows scaling AI and Python workloads from a single machine to a cloud cluster without requiring code changes [previous turn]. This is ideal for orchestrating the diverse steps of data preprocessing into a seamless end-to-end pipeline [previous turn, 571, 577].
- Ray Data for Data Processing: Ray Data provides a "scalable, flexible abstraction for data processing" [previous turn]. It efficiently handles reading data from various formats, transforming datasets, and building data processing pipelines [previous turn, 572]. It can also stream and pipeline data across CPUs and GPUs, effectively reducing idle resources by managing data in memory or intelligently spilling to disk [previous turn].
- "Last Mile" Processing and Featurization: Ray Data is particularly effective for "last mile" processing, which includes crucial steps like basic data loading, cleaning, and featurization before training or inference in machine learning workflows [@pumperla2023learning]
- Parallelization Benefits: Ray's core abstractions, "tasks" and "actors," facilitate efficient parallel execution of tasks such as data ingestion, transformation, and batching [previous turn]. Its use of a shared memory object store minimizes expensive data copying between processes on the same node [previous turn].
- Integration for ML/AI Pipelines: Ray AIR Preprocessors are designed to transform input data into features for ML experiments
- These include various types such as feature scalers, generic preprocessors, categorical encoders, and text encoders
- This directly supports the vectorization and embedding extraction mentioned (e.g., "encoding/decoding tokens to embeddings generation" [previous turn]). Ray AIR enables end-to-end ML applications within a single distributed system, reducing the need to integrate multiple frameworks [previous turn].
By effectively executing these steps, Phase 1 ensures that the data is ready and optimized for the computationally intensive numerical simulation applications in Phase 2.









Genomics for encoding GNA sequencing: LLM or Graph Neuron Netork (tasks of DNA classification, interpretation of structural, )
    pattern mathing over GNN - after encodding (graph vector embeddings)
    visualizie results: NICE DVC to load mesh (from OpenFOAM) in ParaView 
    Genomic benchmarks: a collection of datasets for genomic sequence classification pmc.ncbi.nlm.nih.gov/articles/PMC10150520/

    160-fold acceleration of the Smith-Waterman algorithm using a field programmable gate array (FPGA) pmc.ncbi.nlm.nih.gov/articles/PMC1896180/pdf/1471-2105-8-185.pdf
    https://www.oreilly.com/library/view/basic-applied-bioinformatics/9781119244332/c10.xhtml
    Genomics, High Performance Computing and Machine Learning www.researchgate.net/publication/352869810_Genomics_High_Performance_Computing_and_Machine_Learning
    computationonal exome and genome analysis api.pageplace.de/preview/DT0400.9781498775991_A30884522/preview-9781498775991_A30884522.pdf
        https://www.researchgate.net/publication/320959019_Computational_Exome_and_Genome_Analysis
    https://www.researchgate.net/publication/230846804_Parallel_Iterative_Algorithms_From_Sequential_to_Grid_Computing
        Parallel Iterative Algorithms: From Sequential to Grid Computing books.google.com.pa/books?id=ft7E5hiIzDAC&printsec=frontcover#v=onepage&q=protein&f=false
    basic applied bioinformatics

    https://www.internationalgenome.org/
        https://registry.opendata.aws/1000-genomes/

    References for Dataset 
    registry.opendata.aws
    protein 
        https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter12/protein-secondary-structure-model-parallel.ipynb
        Genomic benchmarks: a collection of datasets for genomic sequence classification
        https://github.com/rieseberglab/fastq-examples
        https://learn.gencore.bio.nyu.edu/ngs-file-formats/fastq-format/





