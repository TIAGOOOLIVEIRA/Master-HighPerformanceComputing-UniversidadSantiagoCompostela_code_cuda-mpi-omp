# Tech-Stack Foundations: Phases 1 & 2 Integration

## Data Deluge in Genomics

The rapid advancements in DNA sequencing have created a true “tsunami of genomic data,” transforming Bioinformatics into a computationally intensive discipline. Traditional Big Data and storage technologies struggle under this load—genomic datasets now span gigabases to terabases per run, and a single human genome can demand around 200 GB. As a result, sequential programs like Velvet require massive memory (≥ 2 TB) and weeks of compute time, while network bottlenecks and I/O constraints can erode any gains from accelerated hardware. Moreover, heterogeneous sequence lengths and iterative algorithms exacerbate load‐balancing issues in parallel applications, making the slowest task set the overall pace.

## Phase 1 Recap: Preparing Data for Simulation

To tame these volumes, Phase 1 pipelines ingest raw FASTQ reads—often via AWS DataSync into S3—then perform quality filtering, error correction (e.g. Spectrum-based Error Correction [5]), and duplicate removal. Clean reads are converted to BAM/SAM, aligned or assembled, and then annotated with ORFs, motifs, and variants. Finally, we extract high-level features (k-mers for DNA, reduced alphabets for proteins) and convert them into dense vectors or embeddings—leveraging models like DNABERT [6] — so that downstream numerical solvers can ingest uniform $N\times D$ arrays instead of raw text.

## Phase 2 Recap: Numeric Simulations on HPC

With embeddings in hand, Phase 2 offloads compute-heavy tasks to HPC: molecular dynamics (where NetCDF — ßa self-describing, array-oriented file format—is used to store simulation inputs and outputs, and GROMACS — a high-performance MD engine — executes the physics kernels), spectral solvers, or graph algorithms all consume those same vector/embeddings shards. Ray tasks archive embeddings to FSx or S3, then dispatch `gmx mdrun` or MPI/Slurm jobs on ParallelCluster. Treating each simulation step—parsing, transformation, force calculation—as its own module enables CI/CD for reproducible MD pipelines, while Ray scales data I/O and orchestration.

## Embedding-Specific Challenges & LSHvec

Embeddings introduce their own hurdles. Unlike natural language vocabularies, DNA k-mers explode combinatorially (up to $4^k$), creating lookup tables tens of times larger than English word tables. Error-induced k-mers—rare and noisy—further dilute training quality when standard embedding models treat them as unique tokens. LSHvec [19] tackles this by bucketing similar k-mers via locality-sensitive hashing, dramatically reducing the effective table size and mitigating noise. Still, training on petabyte-scale corpora (e.g. 500 GB GenBank references + 230 TB generated data) can take weeks even on multi-node HPC, underscoring the need for distributed training frameworks.

## Ray & Anyscale: Gluing Phases 1 and 2

Throughout both phases, Ray (and Anyscale) offers a unified, cloud-agnostic compute layer. In Phase 1, Ray Data pipelines stream and shard data across CPUs/GPUs, applying UDFs for featurization and caching models in long-lived actors for GPU-accelerated inference [14]. In Phase 2, lightweight Ray tasks submit Slurm jobs and manage data staging, so that numerical kernels focus solely on computation. Ray Core’s **tasks** and **actors** abstractions, along with Ray AIR’s end-to-end ML toolkit, eliminate framework sprawl—so you can run ingestion, embedding, training, and simulation in one cohesive Python script.

---

**Conclusion**
By seamlessly looping data and embeddings between Ray-powered preprocessing and HPC-driven simulation, this two-phase pipeline delivers both developer agility and peak performance. The LSHvec case study highlights how embedding-specific innovations pair with distributed orchestration to conquer the scale and complexity of next-generation genomic workflows.