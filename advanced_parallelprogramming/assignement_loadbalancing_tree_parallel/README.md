# Parallelizing findValidMax function in a unbalanced tree

In this task, parallelism approaches using Omp, std:threads and TBB were investigated with the aim of obtaining better performance over the findValidMax function, which performs a search for the largest value of an unbalanced tree in a sequential manner.

To this end, the findValidMaxTBB_SharedPtr method was created where the aforementioned libraries could be used, together with a limited and simplified Actor Model approach.

The main function executes the creation of the tree, then calls the findValidMax and findValidMaxTBB_SharedPtr functions, collecting their respective execution times for benchmarking purposes.

The research work to arrive at the version of findValidMaxTBB_SharedPtr, which is called in main, can be observed for comparison and discovery purposes in the homonymous methods with an additional incremental number to their name for differentiation. But in each of these versions, some new concept could be validated.

The bibliographical references demonstrate at least two strategies to face the scalability problem in operations on an unbalanced tree: adding parallelism can bring performance gains - but it can cause thread overload causing contention, if the tree is highly unbalanced; adding a load balancing technique together with task orchestration - which would require some revision at a greater or lesser level of the application architecture.

This work was quite exhaustive for the first strategy, where libraries for this purpose of parallelism could be tested. And for this, a performance gain was observed when the application was executed on a MacOS (m1) operating system. However, the same implementation in the HPC environment demonstrated a considerable performance degradation.

In both cases and for both processes (sequential and parallel), the result was consistent in the search for the largest numerical value in the tree.

## The requirements

```c
The file http://gac.udc.es/~basilio/ppa_project.cpp contains a sequential program that  
1) generates an unbalanced tree by means of the function generateTree, 
2) finds the maximum value stored in the tree that meets a number of conditions provided by a vector of functions and the path to reach it, which is performed by function findValidMax, and 
3) tests the correctness of the path by means of function getPathVal  Implement a shared memory parallelization of function findValidMax that is based on C++11, oneTBB or a combination of both. The implementation of a load balacing strategy in the code will be particularly valued, given the degree of unbalance of the tree. The implementation should not be based on the concrete properties of the tree or the particular functions used in the example; rather it should be totally generic and suitable for any tree based on nodes of the type Node_t used in the program as well as for any vector of conditions. As a result, it its not valid to use in the implementation statistics gathered from an analysis of the tree. It is not valid either to make any change in the list of condition functions.  The tree generated by the example program requires several Gigabytes of main memory. Thus it is adviseable to execute it reserving 32GB for example; which can be made in the Finisterrae 3 by means of the command compute -c 1 --mem 32.
 You must provide: - Your parallel application. It should receive the number of parallel threads to use by means of the environment variable OMP_NUM_THREADS; as if it had been written using OpenMP. You must only parallelize findValidMax, but you can add new functions, types and/or global variables. - A short explanation of the code / parallelization strategy in English (max 2 pages)
 Please provide both files in a single tar or zip file by the deadline set.
```

## Setup
The investigation was coded on a MacOS (m1) with 32GB of memory as a development environment.
While its consistent compilation and execution was confirmed in this environment, the same application was executed in the HPC FT3 environment according to the resource allocation:
  
  - $ compute -c 1 --mem 64

The application was stopped by the cluster task manager when 32GB of memory was allocated. This justifies the use of 64GB.

A makefile is also provided so that compilation between these two environments occurs transparently.
It is also in this Makefile that the following environment variables are defined:

  - $ export OMP_NUM_THREADS=8
  - $ export TBB_NUM_THREADS=8



## Unbalanced tree assessment
The workload is based on generating and processing a large unbalanced tree. Nodes are unevenly distributed across branches, with the number of children increasing dynamically based on node value and level.

    Tree Size: >500 million nodes.

    Traversal: Full-depth, recursive navigation with complex value-based conditions.

    Challenge: The unbalanced nature causes inefficient task splitting/coordination, idle threads, and poor load distribution.


## Parallelism strategies
Several approaches were benchmarked:
findValidMax (Sequential - original)

    Algorithm: Depth-First Search (DFS).

    Pros: Low overhead, cache-friendly, no synchronization needed.

    Cons: Does not scale with CPU cores.

findValidMaxTBB_SharedPtr (fine-tunned version)

    Original Strategy: Recursive traversal with task_group, task_arena, and shared atomics.

    Advanced Version (limited exploratorial): Actor-style dynamic task queue with batching using tbb::concurrent_queue.

    TBB for actor coordination and work splitting.

    OpenMP for subtree parallelization in heavy branches.

    Hipothesis: Exploits both data-level and task-level parallelism.

## Load Balancing strategies
Limited load distribution methods were explored without a positive result in terms of performance improvement:
1. Task Per Branch

    Fine-grained but leads to oversubscription and overhead.

2. Static Batching

    Batches of N children processed per task

3. Actor-style Queue

    Centralized concurrent_queue where actors pop tasks.

    Enables dynamic load balancing.

    Reduces idle time and improves throughput under imbalance.

## Performance fine-tunning
Key optimizations applied:

    Lock-Free Atomics: For concurrent updates to maxValid and bestPathWithMax.

    Smart Batching: Grouped multiple child tasks into one submission.

    Actor Design: Queued tasks improve utilization under irregular trees.

    Task Granularity: Adjusted to minimize thread scheduling overhead.

    Memory Locality: Reused and minimized allocations for path tracking.

## Profiling 
 VTune Insights

Key Findings (HPC Environment - Intel Xeon, 64 cores):

    Parallel version was 2.3x slower than sequential.

    Logical Core Utilization: 1.5%

    Back-End Bound: 20.9%

    Store-Bound: 8.2% (due to atomic writes)

    Front-End Bound: 17.1% (I-cache + branching issues)

    Poor Task Stealing due to unbalanced task distribution

Recommendations from VTune:

    Reduce contention

    Use fewer, coarser-grained tasks

    Improve memory access patterns

    Avoid deep recursion and improve vectorization

## Analysis
### Performance Summary by Platform

|                          | macOS (M1, 8 Cores) | HPC (Xeon, 64 Cores) |
|--------------------------|---------------------|------------------------|
| `findValidMax`           | Slower              | ~190s                  |
| `findValidMaxTBB`        | Faster              | ~449s                  |
| **Speedup (TBB vs Seq)** | ~1.4×               | ~0.42× (slower)        |

---

### Thread Utilization (VTune)

| Metric                 | Value            |
|------------------------|------------------|
| Logical Core Use       | 1.5% (0.976/64)  |
| Physical Core Use      | 1.5% (0.975/64)  |
| IPC (Instructions/Clk) | 2.530            |

---

### Backend and Memory Bound Analysis

| Pipeline Bound Category   | Percentage of Slots |
|---------------------------|---------------------|
| Retiring                  | 59.4%               |
| Front-End Bound           | 17.1%               |
| Bad Speculation           | 2.6%                |
| Back-End Bound            | 20.9%               |
| &nbsp;&nbsp;&nbsp;&nbsp;- Memory Bound         | 8.7%                |
| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- L1 Bound          | 4.1%                |
| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- L2 Bound          | 0.1%                |
| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- L3 Bound          | 0.2%                |
| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- DRAM Bound        | 1.1%                |
| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Store Bound       | 8.2%                |
| &nbsp;&nbsp;&nbsp;&nbsp;- Core Bound           | 12.3%               |

---

### Instruction Mix and Vectorization

| Category                    | Value         |
|-----------------------------|---------------|
| Scalar FP Ops (SP/DP)       | ~100% / 66.7% |
| Packed FP Ops (DP only)     | 33.3%         |
| Vectorization Ratio         | 28.6%         |
| FP Arith / Mem Read Ratio   | 0.004         |
| FP Arith / Mem Write Ratio  | 0.004         |

---

### Summary of Observed Bottlenecks

| Problem Area                 | VTune Insight                |
|-----------------------------|------------------------------|
| Underutilized Threads       | Only ~1 core used            |
| Load Imbalance              | Unbalanced tree traversal    |
| High Atomic Store Contention| 8.2% Store Bound             |
| Memory Latency              | DRAM Bound + Cache Misses    |
| Fine-Grained Tasks          | High scheduling overhead     |
| Poor ILP / Branchy Code     | Front-End Bound              |

---

### Comparison Summary Table

| Aspect                          | `findValidMax` (Sequential)        | `findValidMaxTBB_SharedPtr` (TBB Parallel)        |
|----------------------------------|------------------------------------|---------------------------------------------------|
| **Execution Time (HPC)**         | ~190s                              | ~449s (2.36× slower)                              |
| **Execution Time (macOS M1)**    | Higher than TBB                    | Lower than Sequential (TBB outperforms)           |
| **Load Balancing Strategy**      | Depth-First Traversal (DFS)        | Dynamic, task-based; partial Rake-style           |
| **Parallelism**                  | None                               | TBB `task_group`, `task_arena`, atomic sync       |
| **Scalability**                  | N/A                                | Low on HPC (1.5% core utilization)                |
| **Speedup**                      | Baseline                           | Negative speedup on HPC                           |
| **Task Granularity**             | Full-tree traversal per call       | Per-node and per-batch parallel execution         |
| **Thread Utilization (VTune)**   | N/A                                | ~1 core out of 64                                 |
| **Work/Resource Sharing**        | Centralized, single-threaded       | Work stealing via TBB, atomic shared state        |
| **Vectorization**                | Minimal                            | Same (28% packed FP ops; mostly scalar)           |



### Why TBB Performs Worse in HPC

    ⚖️ Load Imbalance:

        Tree is highly unbalanced, leading to task starvation.

        Some subtrees spawn many tasks, others almost none.

        TBB's work-stealing fails to spread the workload effectively.

    💸 Threading Overhead:

        Fine-grained tasks cause scheduling overhead.

        task_group incurs more cost than benefit in shallow subtrees.

    🧵 Poor Core Utilization (1.5%):

        Tasks are not long enough to keep threads busy.

        Task spawning faster than task processing.

    🧠 Memory/Store Bound:

        8.7% backend stalls on memory.

        Store-bound 8.2% → frequent atomic contention (for max/path).

    Front-End Bound (17.1%):

        Branch-heavy recursive code with low ILP.

        Tree navigation → high I-cache misses.

## Conclusion
On macOS (M1 architecture), TBB and hybrid versions outperform the sequential implementation due to better core saturation and lighter threading overhead.

On HPC Xeon environments, sequential outperforms due to:

    Load imbalance

    Thread underutilization

    Memory contention and atomic synchronization bottlenecks

Takeaway: For highly unbalanced trees, an actor-style batching system with intelligent load distribution (via task queues) performs more consistently across architectures. Hybrid strategies combining OpenMP and TBB offer promising directions for scalable performance — especially when combined with dynamic work allocation and profiling feedback.

## Future work
The results in the interactive investigation or fine-tuning process shows that by adding parallelism without proper orchestration or load balancing does not guarantee performance—especially on highly irregular data structures like unbalanced trees. Adopting a pure Actor Model approach can bring architectural clarity and adaptive concurrency.

•Dynamic Load Balancing of Unbalanced Computations Using Message Passing
•Data-parallel load balancing strategies
•Strategies for Dynamic Load Balancing on Highly Parallel Computers
•Parallel Programming and High-Performance Computing Part 6: Dynamic Load Balancing
•Dynamic Load Balancing Strategies for Parallel Computers
•Distributed dynamic load balancing for task parallel programming

### Actor Model (Actor Model Architecture Proposal for Tree Traversal)

```text
+-------------------------+
|   Tree Generation       |
+-------------------------+
           |
           v
+-------------------------+
|  Root Actor Spawner     |  <--- creates N Actor Workers
+-------------------------+
           |
           v
+-------------------------+       +--------------------+
|   Task Dispatcher       |-----> |  Actor Queue (N)   |
+-------------------------+       +--------------------+
           |                                 |
           v                                 v
+-------------------------+       +--------------------+
| Load Balancer (Policy) |       |  Actor Worker      |
|  - Round-Robin         |       |  - Pulls tasks     |
|  - Work Stealing       |       |  - Spawns children |
+-------------------------+       +--------------------+
           |                                 |
           v                                 v
+------------------------------------------------------+
|  Processing Backend (Recursive DFS/Batch w/OMP/TBB)  |
+------------------------------------------------------+
           |
           v
+--------------------------+
|  Shared Result Monitor   |  <-- lock-free/optimistic
| - maxValid (atomic)      |
| - bestPathWithMax (atomic)
+--------------------------+





## References


    3. Flow Graphs _ Pro TBB_ C++ Parallel Programming with Threading Building Blocks.pdf
        Future work: redesign the tree as a Flow:graph
    4. TBB and the Parallel Algorithms of the C++ Standard Template Library _ Pro TBB_ C++ Parallel Programming with Threading Building Blocks.pdf
        Vectorization SIMD + Threads 
    “Vector and Wavefront Policies,” Programming LanguageC++ (WG21), P0076r3, 
        http://open-std.org/JTC1/SC22/WG21/docs/papers/2016/p0076r3.pdf , July 7, 2016        
    7. Scalable Memory Allocation _ Pro TBB_ C++ Parallel Programming with Threading Building Blocks.pdf
        Compilation Considerations
    8. Mapping Parallel Patterns to TBB _ Pro TBB_ C++ Parallel Programming with Threading Building Blocks.pdf
        Flof-graph: event based coordination    
    9. The Pillars of Composability _ Pro TBB_ C++ Parallel Programming with Threading Building Blocks.pdf

    15. Cancellation and Exception Handling _ Pro TBB_ C++ Parallel Programming with Threading Building Blocks.pdf
        Advanced Task Cancellation
            tg.cancel_group_execution();
    16. Tuning TBB Algorithms_ Granularity, Locality, Parallelism, and Determinism _ Pro TBB_ C++ Parallel Programming with Threading Building Blocks.pdf
        cache affinity
    18. Beef Up Flow Graphs with Async Nodes _ Pro TBB_ C++ Parallel Programming with Threading Building Blocks.pdf
        Async World Example
    20. TBB on NUMA Architectures _ Pro TBB_ C++ Parallel Programming with Threading Building Blocks.pdf
        likwid-bench -t stream -i 1 -w S0:12GB:16-0:S0,1:S0,2:S0
        Mastering Data Placement and Processor Affinity
    1 Jumping Right In_ Hello TBB _ Pro TBB_ C++ Parallel Programming with Threading Building Blocks.pdf
    c_c-data-structures-and-algorithms-in-c.pdf
        13.2.2 The Adjacency List Structure
    C++_Concurrency_in_Action_Second_Editio.pdf
        Waiting for an event or other condition
            mutex vs Future if check global variable?

    O'Reilly Intel Threading Building Blocks OutFitting C++ for Multi-Core Processor Parallelism.pdf
    Functional_Programming_in_C++.epub
        “1.4.2	Concurrency and synchronization”
        Parallel versions of standard algorithms  
        HPX parallel STL
        http://mng.bz/EBys
        http://mng.bz/8435
